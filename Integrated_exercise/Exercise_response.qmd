---
title: "Integrated excercise"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    self-contained: true
    theme: united
    fontsize: 1em
    code-fold: true
bibliography: references.bib
---

![](work_in_progress.png){fig-align="center" width="25%"}

# Background behind the dataset

This dataset is a simulated dataset that is based on an existing study of @frumuselu2015. In this study, the key question was whether subtitles help in foreign language acquisition. Spanish students (n = 36) watched episodes of the popular tv-show "Friends" for half an hour each week, during 26 weeks. The students were assigned to 3 conditions:

-   English subtitled (condition "FL")

-   Spanish subtitled (condition "MT")

-   No subtitles (condition "NoSub")

At 3 occasions students got a Fluency test:

-   Before the 26 weeks started

-   After 12 weeks

-   After the experiment

The dependent variable is a measure based on the number of words used in a scripted spontaneous interview with a test taker. The data is structured as follows:

```{r}
load(file = "Subtitles.RData")
head(Subtitles, 9)
```

If we visualize the dataset we get a first impression of the effect of the condition. In this exercise it is your task to do the proper Bayesian modelling and interpretation.

```{r}
#| message: false
#| warning: false
#| error: false

library(tidyverse)

theme_set(theme_linedraw() +
            theme(text = element_text(family = "Times", size = 10),
                  panel.grid = element_blank())
)

Subtitles %>%
  ggplot(
    aes(
      x = occasion,
      y = fluency,
      group = student
      )
  ) +
  geom_path(
    aes(
      color = condition
    )
  ) + 
  scale_x_continuous(
    breaks = c(1,2,3)
  )
```

------------------------------------------------------------------------

```{r, out.height = "40%", out.width="40%", echo = FALSE}
knitr::include_graphics("friends3.jpeg")
```

------------------------------------------------------------------------

# 1. Task 1

First we will start by building 4 alternative mixed effects models. In each of the models we have to take into account the fact that we have multiple observations per student. So we need a random effect for students in the model.

-   M0: only an intercept and a random effect for `student`

-   M1: M0 + fixed effect of `occasion`

-   M2: M1 + fixed effect of `condition`

-   M3: M2 + interaction effect between `occasion` and `condition`

Make use of the default priors of `brms`.

Once the models are estimated, compare the models on their fit making use of the leave-one-out cross-validation. Determine which model fits best and will be used to build our inferences.

------------------------------------------------------------------------

<i>Solution</i>

The following code-block shows how the models can be estimated and compared on their fit.

```{r}
#| echo: true
#| eval: false
#| message: false
#| error: false
#| cache: false

library(brms)

M0 <- brm(
  fluency ~ 1 + (1|student),
  data = Subtitles,
  cores = 4,
  backend = "cmdstanr"
)

M1 <- brm(
  fluency ~ 1 + occasion + (1|student),
  data = Subtitles,
  cores = 4,
  backend = "cmdstanr"
)

M2 <- brm(
  fluency ~ 1 + occasion + condition + (1|student),
  data = Subtitles,
  cores = 4,
  backend = "cmdstanr"
)

M3 <- brm(
  fluency ~ 1 + occasion + condition + occasion*condition + (1|student),
  data = Subtitles,
  cores = 4,
  backend = "cmdstanr"
)

loo_M0 <- loo(M0)
loo_M1 <- loo(M1)
loo_M2 <- loo(M2)
loo_M3 <- loo(M3)

loo_models <- loo_compare(
  loo_M0,
  loo_M1,
  loo_M2,
  loo_M3
  )

print(loo_models, simplify = F)
```

```{r}
#| echo: false
#| eval: true

loo_models <- readRDS(file = "loo_models.RDS")

print(loo_models, simplify = F)
```

Based on the model comparison we can conclude that the final model (M3) fits the data best. We will use this model in the next sections to build our inferences.

------------------------------------------------------------------------

# 2. Task 2

Now that we have established the best fitting model, it is our task to approach the model critically before delving into the interpretation of the results.

Apply the different steps of the WAMBS check-list template for the final model.

## Subtask 2.1: what about the priors?

What are the default `brms` priors? Do they make sense? Do they generate impossible datasets? If necessary, specify your own (weakly informative) priors and approach the critically as well.

## Subtask 2.2: did the model converge properly?

Perform different checks on the convergence of the model.

## Subtask 2.3: does the posterior distribution histogram have enough information?

Check if the posterior distribution histograms of the different parameters are informative enough to substantiate our inferences.

## Subtask 2.4: how wel does the model predict the observed data?

Perform posterior predictive checks based on the model.

## Subtask 2.5: what about prior sensitivity of the results?

Finally, we have to check if the results of our model are not to dependent on the priors we specified in the model.

# 3. Task 3

# References
